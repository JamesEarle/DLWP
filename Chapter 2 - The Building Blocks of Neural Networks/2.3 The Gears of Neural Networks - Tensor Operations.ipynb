{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 The Gears of Neural Networks - Tensor Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.1 Element-wise Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider the below function a naive implementation of relu\n",
    "# This is essentially what a Dense layer with activation='relu' would be doing.\n",
    "# e.g. keras.layers.dense(512, activation='relu')\n",
    "# e.g. output = relu(dot(W, input) + b)\n",
    "# We have a dot product of two tensors (W, input), an addition of 2D tensor with vector b, and relu which is just max(x, 0)\n",
    "\n",
    "def naive_relu(x):\n",
    "    assert len(x.shape) == 2\n",
    "    \n",
    "    x = x.copy\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] = max(x[i, j], 0)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can do the same with addition, and other basic operations\n",
    "\n",
    "def naive_add(x, y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert x.shape == y.shape\n",
    "    \n",
    "    x = x.copy\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] += y[i, j]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fortunately these exist as well optimized operations in Python and packages like Numpy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# element-wise addition, equivalent to above\n",
    "def add(x, y):\n",
    "    return x + y\n",
    "\n",
    "# element-wise relu, equivalent to above\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.2 Broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With our Dense layer before we added a 2D tensor to a vector, but our naive add only supports 2d tensors of the same shape.\n",
    "# How do we adjust for different tensor shapes?\n",
    "# We can broadcast, if possible, to resolve this by adding dimensions to the smaller tensor to match the larger one\n",
    "# then we repeat the tensor alongside these new axes to match the full shape of larger tensors\n",
    "# new_Y[i, :] == old_y after this is done\n",
    "\n",
    "def naive_add_matrix_and_vector(x, y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert len(y.shape) == 1\n",
    "    # Vector must be as long as the matrix is wide for addition to be feasible\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    \n",
    "    z = np.zeros(x.shape[0])\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            z[i] += x[i, j] * y[j]\n",
    "    return z\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.93518481 0.26755541]\n",
      " [0.88230601 0.98446822]\n",
      " [0.233801   0.99898316]]\n"
     ]
    }
   ],
   "source": [
    "x = np.random.rand(3, 2)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.62701281 0.8203303 ]\n"
     ]
    }
   ],
   "source": [
    "y = np.random.rand(2,)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "z = np.zeros(x.shape[0])\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.80585666 1.36080628 0.96609238]\n"
     ]
    }
   ],
   "source": [
    "z = naive_add_matrix_and_vector(x, y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 3, 32, 10)\n"
     ]
    }
   ],
   "source": [
    "# Generally you can apply two tensor element-wise operations if one tensor has shape\n",
    "# (a, b, ... n, n+1, ... m) and the other has shape (n, n+1, ... m)\n",
    "# The broadcasting will automatically happen for axes a through n-1\n",
    "# e.g. below\n",
    "x = np.random.random((64, 3, 32, 10))\n",
    "y = np.random.random((32, 10))\n",
    "\n",
    "z = np.maximum(x, y)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.3 Tensor Dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.68506263 3.31406718 3.78618941 2.91334889 2.89658944 2.68711702\n",
      "  2.72080266 3.91639288 3.03898946 3.51045239 2.99665752 2.69715989\n",
      "  2.77307728 3.40303366 3.46792519 2.7364031  2.65233852 2.9082524\n",
      "  2.83816384 2.72600704 3.223352   2.32999656 3.80403233 2.28046711\n",
      "  2.54219896 2.75399334 3.13919208 2.8065087  2.09730971 3.44830624\n",
      "  2.01912892 1.97303338]\n",
      " [2.67441819 3.03880549 3.20581212 2.13772117 3.43112122 2.26780449\n",
      "  2.97495391 3.41085141 2.66971216 3.15754697 3.17190367 2.46242681\n",
      "  2.99725517 3.56114378 3.11727866 2.75202033 1.77377784 2.5536478\n",
      "  2.64962949 2.14115705 3.06736666 2.17089724 3.28569394 2.05690939\n",
      "  2.46163852 2.22251122 2.832659   2.1932295  2.17501999 3.23738712\n",
      "  1.83474276 1.54459735]\n",
      " [2.56344727 3.03318593 3.48801441 2.82725123 3.29654152 2.05170162\n",
      "  3.14051    3.46050501 2.9664575  3.52196908 3.71356589 2.71625683\n",
      "  2.8165592  3.55829951 2.76154499 2.20191054 2.20997216 2.92111193\n",
      "  2.53164638 3.16500459 3.0233204  2.49809725 3.30720446 2.64084507\n",
      "  2.34330588 3.33668944 3.3207475  2.64467548 2.11153231 3.80269886\n",
      "  2.56940084 2.38029875]]\n"
     ]
    }
   ],
   "source": [
    "# Also called tensor product, not the same as element-wise product though\n",
    "x = np.random.random((32, 10))\n",
    "y = np.random.random((10, 32))\n",
    "\n",
    "z = np.dot(x, y)\n",
    "print(z[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8965211159609283\n"
     ]
    }
   ],
   "source": [
    "def naive_vector_dot(x, y):\n",
    "    # vectors must be the same size to be used in dot product\n",
    "    assert len(x.shape) == 1\n",
    "    assert len(y.shape) == 1\n",
    "    assert x.shape[0] == y.shape[0]\n",
    "    \n",
    "    z = 0\n",
    "    for i in range(x.shape[0]):\n",
    "        z += x[i] * y[i]\n",
    "    return z # returns a scalar\n",
    "\n",
    "x = np.random.random((10))\n",
    "y = np.random.random((10))\n",
    "\n",
    "z = naive_vector_dot(x, y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 5) (5,)\n"
     ]
    }
   ],
   "source": [
    "# Can also do dot product of matrix x and vector y\n",
    "\n",
    "def naive_matrix_vector_dot(x, y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert len(y.shape) == 1\n",
    "    \n",
    "    # Vector length must be same as matrix width\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    \n",
    "    # Answer has shape equal to number of rows in matrix x\n",
    "    z = np.zeros(x.shape[0])\n",
    "    for i in range(x.shape[0]):\n",
    "        # can also do z[i] = naive_vector_dot(x[i, :], y)\n",
    "        for j in range(x.shape[1]):\n",
    "            z[i] += x[i, j] * y[j]\n",
    "    return z\n",
    "    \n",
    "\n",
    "x = np.random.random((3, 5))\n",
    "y = np.random.random((5))\n",
    "\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.74861648 1.40270977 1.1569119 ]\n"
     ]
    }
   ],
   "source": [
    "z = naive_matrix_vector_dot(x, y)   \n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3) [[0.50479263 0.90559633 0.85328296]\n",
      " [0.0825802  0.53431785 0.6635519 ]]\n"
     ]
    }
   ],
   "source": [
    "# When a tensor's ndim > 1, note that dot is no longer symmetric (x . y != y . x)\n",
    "# You can take the dot product of two matrixes x and y if and only if x.shape[1] = y.shape[0] (x_num_cols == y_num_rows)\n",
    "# The results is a matrix with shape (x.shape[0], y.shape[1])\n",
    "\n",
    "def naive_matrix_dot(x, y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert len(y.shape) == 2    \n",
    "    \n",
    "    # x_rows == y_cols\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    z = np.zeros((x.shape[0], y.shape[1]))\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(y.shape[1]):\n",
    "            row_x = x[i, :]\n",
    "            col_y = y[:, j]\n",
    "            \n",
    "            z[i, j] = naive_vector_dot(row_x, col_y)\n",
    "    return z\n",
    "\n",
    "x = np.random.random((2, 3))\n",
    "y = np.random.random((3, 4))\n",
    "\n",
    "print(x.shape, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 4) [[0.74244733 0.34011211 0.14474552 0.81383989]\n",
      " [0.20085238 0.25385649 0.51022188 0.33615547]\n",
      " [0.87909284 0.32736118 0.00525729 0.96755869]]\n"
     ]
    }
   ],
   "source": [
    "print(y.shape, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 4) [[1.30678806 0.68090931 0.53960749 1.54084288]\n",
      " [0.75195419 0.38094772 0.28806226 0.88884634]]\n"
     ]
    }
   ],
   "source": [
    "z = naive_matrix_dot(x, y)\n",
    "print(z.shape, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.4 Tensor Reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[0, 1], [2, 3], [4, 5]])\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [2],\n",
       "       [3],\n",
       "       [4],\n",
       "       [5]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.reshape(6,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2],\n",
       "       [3, 4, 5]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.reshape(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [2, 3],\n",
       "       [4, 5]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transposing, exchanging rows and columns so that x[i, :] becomes x[:, i]\n",
    "x = np.array([[0, 1], [2, 3], [4, 5]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 2, 4],\n",
       "       [1, 3, 5]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.transpose(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
